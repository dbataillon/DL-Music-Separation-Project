\section{Data}

We conduct our experiments using the DSD100 (Demixing Secrets Dataset), a widely-used benchmark for music source separation research. The dataset contains 100 professionally produced songs with isolated multi-track stems, split evenly into 50 development and 50 test tracks. Due to computational constraints, we use only the development subset (50 tracks) for both training and evaluation. The raw audio data occupies approximately 15 GB of storage, with each track averaging around 4 minutes in duration for a total of roughly 3.3 hours of music.

Each track in DSD100 provides a stereo mixture along with four corresponding source stems: \textit{vocals} (lead and backing vocals), \textit{bass} (bass guitar and synthesized bass), \textit{drums} (drum kit and percussion), and \textit{other} (all remaining instruments including guitars, keyboards, and synthesizers). This four-stem decomposition represents a standard formulation for the music source separation task, though the ``other'' category is notably heterogeneous and typically the most challenging to separate.

For preprocessing, we convert all stereo audio to mono and downsample from the original 44.1 kHz to 16 kHz to reduce computational requirements. We compute magnitude spectrograms using a Short-Time Fourier Transform (STFT) with a window size of 1024 samples and a hop length of 768 samples, yielding 513 frequency bins per frame. The magnitude values are normalized to the range $[0, 1]$ using min-max scaling. To create training samples, we extract fixed-size patches of 128 time frames (approximately 6 seconds of audio) using a sliding window with a stride of 10 frames. This process yields 26,478 training patches from the 50 development tracks.

We do not employ a separate validation or test split; all 50 development tracks are used for training, and evaluation metrics are computed on the same tracks. While this does not allow us to assess generalization to unseen data, it enables direct comparison of model capacity and training dynamics given identical data. The pretrained Demucs baseline was trained on a large external proprietary dataset and serves as an upper-bound reference for achievable performance.

The DSD100 dataset has several known limitations. The relatively small size of 50 training tracks is insufficient for modern deep learning approaches, which typically benefit from thousands of training examples. The dataset exhibits genre bias, consisting predominantly of Western pop and rock music, which may limit generalization to other musical styles. Additionally, some tracks contain minor stem leakage where audio from one source bleeds into another stem due to recording conditions. We do not apply data augmentation techniques such as random gain adjustment, pitch shifting, or time stretching, though such augmentations could potentially improve model robustness and are left for future work.
