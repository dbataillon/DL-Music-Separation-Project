\begin{abstract}
Music source separation---isolating individual instruments from mixed recordings---enables applications such as remixing, transcription, and hearing-aid enhancement. While state-of-the-art systems use deterministic convolutional or transformer architectures, we investigate whether denoising diffusion probabilistic models can improve separation quality by leveraging their capacity to model complex distributions. Our approach, \emph{FreqNet}, conditions a U-Net diffusion model on mixture spectrograms to iteratively denoise and reconstruct isolated stems for vocals, drums, bass, and other instruments. We evaluate on the DSD100 benchmark and conduct ablations on spectrogram representations ($\epsilon$-prediction vs.\ v-prediction, linear vs.\ log-magnitude scaling). Our experiments reveal that diffusion models struggle with the extreme sparsity of magnitude spectrograms, achieving $-16.69$ dB average SDR with our best variant (v-prediction, log-magnitude) compared to $-0.23$ dB for a deterministic U-Net baseline. In contrast, a waveform-domain Demucs model trained on identical data achieves $-8.61$ dB, with drums reaching positive SDR ($+4.56$ dB). A pretrained Demucs reference achieves $+11.41$ dB, highlighting the critical role of training data scale. Our analysis identifies spectrogram sparsity as the primary failure mode and suggests that latent-space or mask-based diffusion may be more suitable for this domain.
\end{abstract}
